# -*- coding: utf-8 -*-
"""fomc_nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J4Ri6v9jQpAQUxnnZvEckecl9OcGzHda

Using spaCy and NLTK along with a Bag of Words approach, automated the process of combing through FOMC minutes transcripts to determine sentiment over time and to understand its fed funds predictabilty.
"""

!pip install BeautifulSoup4
!pip install lxml
!pip install html5lib
!pip install nltk==3.2.5
!pip install pdfplumber
!pip install -U spacy
!python -m spacy download en
!pip install yfinance

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from collections import Counter
from nltk.corpus import stopwords
import spacy
from html import unescape
import pdfplumber
from datetime import datetime 
import datetime as dt

import urllib
import urllib.request
from bs4 import BeautifulSoup
from bs4 import re
import requests
import re
import yfinance as yf
from statsmodels.tsa.stattools import adfuller
from google.colab import files

# create a spaCy tokenizer

spacy.load('en')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nlp = spacy.lang.en.English()
sp = spacy.load('en_core_web_sm')
all_stopwords = sp.Defaults.stop_words
all_stopwords |= {'the','is','th','s', 'm','would'}

# remove html entities from docs and set everything to lowercase
def my_preprocessor(doc):
  return(unescape(doc).lower())

# tokenize the doc and lemmatize its tokens
def my_tokenizer(doc):
    
    text = word_tokenize(doc)
    tokens_without_sw= [word for word in text if not word in all_stopwords]
    
    return tokens_without_sw

def preprocess_tokens(tokens):
  '''
  Remove any extra lines, non-letter characters, and blank quotes
  '''
  remove_new_lines = [re.sub('\s+', '', token) for token in tokens] 
  #Remove non letter characters
  non_letters = [re.sub('[^a-zA-Z]', '', remove_new_line) for remove_new_line in remove_new_lines]
  #Remove distracting single quotes
  remove_quotes = [re.sub("\'", '', non_letter) for non_letter in non_letters]
  #Removes empty strings from a list of strings
  final = list(filter(None, remove_quotes)) 
  
  return final

def concat_text(text):
    x=''
    for i in text:
        x = x+i
    return x

def get_text(path):  #file is the path
  docs = []
  with pdfplumber.open(path) as pdf:
    for i in range(len(pdf.pages)):
      page = pdf.pages[i]   
      text = page.extract_text()
      docs.append(text)
    concat = concat_text(docs)
  return concat
  
def get_words(full_text):

  raw = [word.lower() for word in full_text.split()]
 
  values = ','.join(map(str, raw))  #converts bytes object to string
  tokenizer = my_tokenizer(values)
 
  words = preprocess_tokens(tokenizer)
  # remove stopwords
  stops = nltk.corpus.stopwords.words('english')
  new_stopwords = ['chairman','would', 'mr']
  stops.extend(new_stopwords)
  words = [word for word in words if word not in stops]
  counter = Counter()
  counter.update(words)
  most_common = counter.most_common(25) 

  return words, most_common

#testing pdf extraction

link = '/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/feddata/2009/FOMC20090812meeting.pdf'
link2 = '/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/feddata/1980/FOMC19800109meeting.pdf'
link3 = '/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/feddata/2009/FOMC20090128meeting.pdf'

text = (get_text(link3))

len(text)
# print(text)

"""### Grab all the files and extract the text from the pdf"""

#Pull the files

# generates a dictionary of transcript paths
# if we already have the pdf data, set path_to_local_pdf to True. 
link_to_file_on_website = False
path_to_local_pdf = True

if link_to_file_on_website:
    base_url = "https://www.federalreserve.gov/monetarypolicy/"
if path_to_local_pdf or path_to_local_txt:
    base_directory = "/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/feddata/"
    
transcript_links = {}
for year in range(1980, 2015): 
    
    if link_to_file_on_website:
        path = "fomchistorical" + str(year) + ".htm"
        html_doc = requests.get(base_url + path)
        soup = BeautifulSoup(html_doc.content, 'html.parser')
        links = soup.find_all("a", string=re.compile('Transcript .*'))
        link_base_url = "https://www.federalreserve.gov"
        transcript_links[str(year)] = [link_base_url + link["href"] for link in links]
        
    elif path_to_local_pdf or path_to_local_txt:
        files = []
        path_to_folder = base_directory + str(year)
        new_files = os.walk(path_to_folder)
        for file in new_files:
            for f in file[2]:
                if path_to_local_pdf:
                    if f[-11:] == "meeting.pdf":
                      files.append(str(file[0]) + "/" + f)
        transcript_links[str(year)] = files
    print("Year Complete: ", year)

Dates, transcripts, all_words, top_tokens = [],[],[],[]

#transcript_links contains every pdf in each year
for year in transcript_links: #produces the year index folder 

  if int(year)<2010:continue
  # if int(year) == 2010: break #test out one decade - opted to end study at 2010

  for file in transcript_links[year]:  #the file in each year folder
    print(file)
    text = get_text(file)
    words = get_words(text)
    
    #Append datapoints to respective lists
    Dates.append(datetime.strptime(file[-19:-11], '%Y%m%d').date())
    transcripts.append(text)  #all pages?
    all_words.append(words[0])  #returns all words in the document
    top_tokens.append(words[1]) #tuple of words with their respective counts in that document
 
#Formatting data in to dataframe
Data = pd.DataFrame([Dates,transcripts, all_words, top_tokens]).T
Data.columns =['Date','Transcript','all_words', 'top_tokens']
Data.set_index('Date', inplace= True)
Data.sort_index(inplace=True)

#save our dataset down for later analysis

Data.to_csv('/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/fomc_transcript_tokens', header=True)

"""To preserve the exact structure of the DataFrame, an easy solution 
is to serialize the DF in pickle format with pd.to_pickle, instead 
of using csv, which will always throw away all information about data types, 
and will require manual reconstruction after re-import. One drawback of pickle is 
that it's not human-readable.
repr() and ast.literal_eval(); for just lists, tuples and integers since 
the csv format converts everything to string.
"""

from ast import literal_eval  

#reload our transcript tokens csv
Data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/fomc_transcript_tokens')

# Data['Transcript'] = Data['Transcript'].apply(literal_eval)
Data['all_words'] = Data['all_words'].apply(literal_eval)
Data['top_tokens'] = Data['top_tokens'].apply(literal_eval)
Data['Date'] = pd.to_datetime(Data['Date'], format='%Y-%m-%d')
Data.set_index("Date", inplace = True)

Data.index

#Count the total word frequency across all transcripts
word_df = pd.DataFrame(columns=['Words', 'Count'])
for i in range(len(Data)):  #total links
  word_count = {'Words':[],'Count':[]}
  for sets in Data['top_tokens'][i]:
    # print(files)  #total pairs
    word_count['Words'].append(sets[0])  #total words broken out
    word_count['Count'].append(sets[1])  #total count per word
    
  word_df1 = pd.DataFrame(word_count)
  word_df = word_df.append(word_df1, ignore_index=True)

total_words = word_df.groupby(['Words']).sum()
sorted_top_words = total_words.sort_values(by='Count', ascending=False)
sorted_top_words=sorted_top_words[:25]

sorted_top_words

import matplotlib.pyplot as plt

sorted_top_words = sorted_top_words.sort_values(by="Count")


plt.rc('axes', facecolor='#E6E6E6', edgecolor='none',
              axisbelow=True, grid=True)
plt.rc('grid', color='w', linestyle='solid')

ax = sorted_top_words.plot(kind='barh', figsize=(8, 10), color='#86bf91', width=0.85);

ax.get_legend().remove()
plt.title("FOMC Minutes - Frequently used words from 1980 to 2010",fontsize=12, weight='bold')
# Draw vertical axis lines
vals = ax.get_xticks()
for tick in vals:
    ax.axvline(x=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)

# Set axis labels

ax.yaxis.label.set_visible(False)
ax.set_xlabel("Total number of occurences", labelpad=20, weight='bold', size=10)

plt.savefig('fomc_top_words.png',dpi=60, bbox_inches = "tight")
files.download('fomc_top_words.png');

"""Build a dictionary:  Applying a sentiment analysis to the words in the documents using the Loughran-McDonald context-specific lexicon, which assigns a simple positive or negative value to words based on the financial services industry context"""

#Create a matrix of word types and the words that match these types
word_list = []
for sentiment_class in ["Negative", "Positive", "Uncertainty",
                       "StrongModal", "WeakModal", "Constraining"]:
    sentiment_list = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/LM Word List.xlsx", sheet_name=sentiment_class,header=None)
    sentiment_list.columns = [sentiment_class]
    sentiment_list[sentiment_class] = sentiment_list[sentiment_class].str.lower()
    word_list.append(sentiment_list)
word_list = pd.concat(word_list, axis=1, sort=True).fillna(" ")
print(word_list.head())
word_list = word_list.to_dict('list')  #create a dictionary out of the excel list and use it to map the transcripts
print(word_list)

negate = ["aint", "arent", "cannot", "cant", "couldnt", "darent", "didnt", "doesnt", "ain't", "aren't", "can't",
          "couldn't", "daren't", "didn't", "doesn't", "dont", "hadnt", "hasnt", "havent", "isnt", "mightnt", "mustnt",
          "neither", "don't", "hadn't", "hasn't", "haven't", "isn't", "mightn't", "mustn't", "neednt", "needn't",
          "never", "none", "nope", "nor", "not", "nothing", "nowhere", "oughtnt", "shant", "shouldnt", "wasnt",
          "werent", "oughtn't", "shan't", "shouldn't", "wasn't", "weren't", "without", "wont", "wouldnt", "won't",
          "wouldn't", "rarely", "seldom", "despite", "no", "nobody"]

def negated(word):
    """
    Determine if preceding word is a negation word
    """
    if word.lower() in negate:
        return True
    else:
        return False

def count_with_negation(fin_dict, transcript):
    """
    Count positive and negative words with negation check. Account for simple negation only for positive words.
    negation is occurring within three words preceding a positive words.
    """
    pos_count = 0
    neg_count = 0
 
    pos_words = []
    neg_words = []

    input_words = re.findall(r'\b([a-zA-Z]+n\'t|[a-zA-Z]+\'s|[a-zA-Z]+)\b', transcript.lower())
 
    word_count = len(input_words)
  

    for i in range(0, word_count):
      if input_words[i] in fin_dict['Negative']:
       
        neg_count += 1
        neg_words.append(input_words[i])
      if input_words[i] in fin_dict['Positive']:
        if i >= 3:
          if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):
            neg_count += 1
            neg_words.append(input_words[i] + ' (with negation)')
          else:
            pos_count += 1
            pos_words.append(input_words[i])
        elif i == 2:
          if negated(input_words[i - 1]) or negated(input_words[i - 2]):
            neg_count += 1
            neg_words.append(input_words[i] + ' (with negation)')
          else:
              pos_count += 1
              pos_words.append(input_words[i])
        elif i == 1:
          if negated(input_words[i - 1]):
                neg_count += 1
                neg_words.append(input_words[i] + ' (with negation)')
          else:
                pos_count += 1
                pos_words.append(input_words[i])
        elif i == 0:
              pos_count += 1
              pos_words.append(input_words[i])
 
    results = [word_count, pos_count, neg_count, pos_words, neg_words]
 
    return results

temp = [count_with_negation(word_list,x) for x in Data.Transcript]
temp = pd.DataFrame(temp)

Data['wordcount'] = temp.iloc[:,0].values
Data['NPositiveWords'] = temp.iloc[:,1].values
Data['NNegativeWords'] = temp.iloc[:,2].values
Data['Poswords'] = temp.iloc[:,3].values
Data['Negwords'] = temp.iloc[:,4].values

temp.head()

plt.rcParams["figure.figsize"] = (18,9)
plt.style.use('fivethirtyeight')

fig, ax = plt.subplots()

ax.plot(Data.index, Data['NPositiveWords'], 
         c = 'green',
         linewidth= 1.0)

plt.plot(Data.index, Data['NNegativeWords'], 
         c = 'red',
         linewidth=1.0)

plt.title('Highly correlated use of both good and bad words')

plt.legend(['Count of Positive Words', 'Count of Negative Words'],
           prop={'size': 20},
           loc = 2
           )

# format the ticks
# round to nearest years.
import matplotlib.dates as mdates
years = mdates.YearLocator()   # every year
months = mdates.MonthLocator()  # every month
years_fmt = mdates.DateFormatter('%Y')

datemin = np.datetime64(Data.index[0], 'Y')
datemax = np.datetime64(Data.index[-1], 'Y') + np.timedelta64(1, 'Y')
ax.set_xlim(datemin, datemax)

# format the coords message box
ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')
ax.grid(True)

plt.savefig('fomc_correlated_words.png',dpi=60, bbox_inches = "tight")
files.download('fomc_correlated_words.png');

plt.show()

NetSentiment = (Data['NPositiveWords'] - Data['NNegativeWords'])

fig, ax = plt.subplots()

ax.plot(Data.index, NetSentiment, 
         c = 'red',
         linewidth= 1.0)

plt.title('Net sentiment implied by BoW over time',size = 'medium')

# format the ticks
# round to nearest years.
datemin = np.datetime64(Data.index[0], 'Y')
datemax = np.datetime64(Data.index[-1], 'Y') + np.timedelta64(1, 'Y')
ax.set_xlim(datemin, datemax)

# format the coords message box
ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')
ax.grid(True)

plt.savefig('fomc_net_sentiment.png',dpi=60, bbox_inches = "tight")
files.download('fomc_net_sentiment.png');

plt.show()

firstderivative = (NetSentiment.shift(1) / NetSentiment)

fig, ax = plt.subplots()

ax.plot(Data.index, firstderivative, 
         c = 'red')

plt.title('Change in sentiment over time (first derivative)')

# format the ticks
# round to nearest years.
datemin = np.datetime64(Data.index[0], 'Y')
datemax = np.datetime64(Data.index[-1], 'Y') + np.timedelta64(1, 'Y')
ax.set_xlim(datemin, datemax)

# format the coords message box
ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')
ax.grid(True)

plt.savefig('fomc_sentiment_chg.png',dpi=60, bbox_inches = "tight")
files.download('fomc_sentiment_chg.png');

plt.show()

#1979 Iranian Oil Crisis 
#https://en.wikipedia.org/wiki/1979_oil_crisis
Oil = np.logical_and(Data.index > '1980-01',
                          Data.index < '1982-11'
                          )

#Black Monday and the time period till US Equity market recovery
#https://en.wikipedia.org/wiki/Black_Monday_(1987)
BlkMonday = np.logical_and(Data.index > '1987-10',
                          Data.index < '1989-09'
                          )
#1994-1995 Mexican Peso crisis
#https://en.wikipedia.org/wiki/1998_Russian_financial_crisis
Peso = np.logical_and(Data.index > '1994-12',
                       Data.index < '1995-08'
                       )

#1998–1999 Russian Ruble crisis
#https://en.wikipedia.org/wiki/1998_Russian_financial_crisis
Russian = np.logical_and(Data.index > '1998-08',
                       Data.index < '1999-08'
                       )

#Dot-com bubble
#https://en.wikipedia.org/wiki/Dot-com_bubble
DotCom = np.logical_and(Data.index > '2000-03',
                         Data.index < '2002-10'
                        )

#Financial crisis of 2007–2008
#https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%932008
Credit = np.logical_and(Data.index > '2007-04',
                         Data.index < '2009-03'
                        )


Crisis = np.logical_or.reduce( ( Oil,BlkMonday,Peso, Russian,DotCom,Credit) )

Window = 8
CompToMA = NetSentiment.rolling(Window).mean()

fig, ax = plt.subplots()
ax.plot(Data.index,
         CompToMA,
         c = 'r',
         linewidth= 2)

ax.plot(Data.index, NetSentiment, 
         c = 'green',
         linewidth= 1,
         alpha = 0.5)

# format the ticks
ax.xaxis.set_major_locator(years)
ax.xaxis.set_major_formatter(years_fmt)
ax.xaxis.set_minor_locator(months)
# round to nearest years.
datemin = np.datetime64(Data.index[0], 'Y')
datemax = np.datetime64(Data.index[-1], 'Y') + np.timedelta64(1, 'Y')
ax.set_xlim(datemin, datemax)

# format the coords message box
ax.format_xdata = mdates.DateFormatter('%Y-%m-%d')
ax.grid(True)


plt.title( str('Moving average of last ' + str(Window) + ' statements (~1 Year Window) coicides with periods of economic uncertainty / systemic risk:'))

ax.legend([str(str(Window) + ' statement MA'), 'Net sentiment of individual statements'],
           prop={'size': 20},
           loc = 2
          )

import matplotlib.transforms as mtransforms
trans = mtransforms.blended_transform_factory(ax.transData, ax.transAxes)
theta = 0.9
ax.fill_between(Data.index, 0, 10, where = Crisis,
                facecolor='grey', alpha=0.5, transform=trans)

xs = Data.index
ys = NetSentiment
ax.annotate('1979 Iranian \n Oil Crisis',
            (mdates.date2num(xs[2]), -800))
ax.annotate('Black \n Monday',
            (mdates.date2num(xs[66]), -600))
ax.annotate('Mexican \nPeso\nCrisis',
            (mdates.date2num(xs[122]), -800))
ax.annotate('Russian \nRuble',
            (mdates.date2num(xs[152]), -875))
ax.annotate('Dot-Com',
            (mdates.date2num(xs[165]), -900))
ax.annotate('US Financial \nCrisis',
            (mdates.date2num(xs[222]), -1250))


plt.savefig('fomc_crisis_periods.png',dpi=60, bbox_inches = "tight")
files.download('fomc_crisis_periods.png');
plt.show()

#Retrive historical price data for the SP500
from datetime import datetime, timedelta
ticker = '^GSPC'

window = 250
start = Data.index[0] - timedelta(days=window)
end = Data.index[-1]
market = yf.download(ticker, start=start, end=end, auto_adjust=False)

#Convert to the daily changes and smooth out data with a 250day trailing ma
market_norm = market['Adj Close']
# market_norm = market_norm.diff()
market_norm = market_norm.rolling(window=250).mean() #250day trailing average window
market_norm.dropna(inplace=True)

#Need to fix the x axis to give both series the same length
resampled = market_norm.reindex(NetSentiment.index, method='ffill')

import warnings
pd.set_option('display.float_format', lambda x: '%.3f' % x)

df = pd.concat([NetSentiment,resampled], axis=1)
df.rename(columns={0: 'Sentiment','Adj Close':'S&P500'}, inplace=True)
df['S&P500'] = df['S&P500'].diff()
df = df.dropna(axis = 0, how ='any')
# df['S&P500']=df['S&P500'].fillna(0)
n_mos = df['S&P500'].shape[0]

try:
  return_per_period = (df['S&P500']+1).prod()**(1/n_mos) - 1
except:
  return_per_period = 0

df.head()

# Plot two lines with different scales on the same plot

import matplotlib.ticker as ticker
# formatter = ticker.StrMethodFormatter('{x:,.0f}')

fig = plt.figure(figsize=(12, 5))
line_weight = 3
alpha = .5
ax1 = fig.add_axes([0, 0, 1, 1])
ax2 = fig.add_axes()
# This is the magic that joins the x-axis
ax2 = ax1.twinx()
lns1 = ax1.plot(df['Sentiment'], color='blue', lw=line_weight, alpha=alpha, label='Sentiment')
lns2 = ax2.plot(df['S&P500'], color='green', lw=line_weight, alpha=alpha, label='S&P500 Change')

market_corr = df['S&P500'].corr(df['Sentiment'])

leg = lns1 + lns2
labs = [l.get_label() for l in leg]
ax1.legend(leg, labs, loc='best')
plt.title('Net Sentiment / Net S&P 500 returns 250 dma', fontweight='bold',fontsize=18)
plt.suptitle('Correlation: {}'.format(round(market_corr,3), fontsize=12))

ax1.set(ylabel="Net Sentiment")  
ax2.set(ylabel="S&P Net Change 250d")
ax1.set(xlabel= "\nFOMC Meeting Date")

plt.savefig('sentiment_corr.png',dpi=60, bbox_inches = "tight")
files.download('sentiment_corr.png');
plt.show()

#Dickey Fuller test - helper function to check for stationarity in our time series

def adf_test(series,title=''):
    """
    Pass in a time series and an optional title, returns an ADF report
    """
    print(f'Augmented Dickey-Fuller Test: {title}')
    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data
    
    labels = ['ADF test statistic','p-value','# lags used','# observations']
    out = pd.Series(result[0:4],index=labels)

    for key,val in result[4].items():
        out[f'critical value ({key})']=val
        
    print(out.to_string())   # .to_string() removes the line "dtype: float64"
    
    if result[1] <= 0.05:
       
        print("Reject the null hypothesis")
        print("Data has no unit root and is stationary")
    else:
      
        print("Fail to reject the null hypothesis")
        print("Data has a unit root and is non-stationary")

adf_test(df['Sentiment'], title='Sentiment signal')

adf_test(df['S&P500'], title='S&P')

"""While I used first differencing earlier, it helps to check if the datasets still show stationarity.  The above results indicate both time series have no unit root and we can safely apply Granger Causality"""

#does the sentiment signal help predict stock prices?

from statsmodels.tsa.stattools import grangercausalitytests
data = df[['S&P500','Sentiment']]


print("***Sentiment signal***")
grangercausalitytests(data,maxlag=3);
print('_______________________________________')

"""The Granger Causality indicates that our sentiment signal could predict stock price movement in future periods.  Granger causality means that past values of x2(the sentiment signal) have a statistically significant effect on the current value of x1(the S&P500 Index), taking past values of x1 into account as regressors. We reject the null hypothesis that x2 does not Granger cause x1 if the pvalues are below a desired size of the test. In our case, the F statistic is quite large and our p-value indicated our relationship test was signficant, so we can reject the null hypothesis and believe there has been some causality and potential predictive power to our relationship.  """

#save our current working dataset
df.to_csv('/content/drive/MyDrive/Colab Notebooks/Projects/FOMC_NLP/temp_spx_corr', 
          header=True)

